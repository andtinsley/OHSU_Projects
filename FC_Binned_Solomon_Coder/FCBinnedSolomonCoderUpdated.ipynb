{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "path = os.getcwd()\n",
    "    # read all the files with extension .csv\n",
    "filenames = glob.glob(os.path.join(path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    getBucket = int(input(\"Enter the size of bins: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all subject files\n",
    "subjectFiles = []\n",
    "for subjectFile in filenames:\n",
    "    getFilename = subjectFile.rsplit('\\\\')[-1]\n",
    "    getFilename = getFilename.split(\".\")[0]\n",
    "    getFilename = getFilename.split(\"-\")[0]\n",
    "    subjectFiles.append(getFilename)\n",
    "subjectFiles = np.unique(subjectFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'INC03_Day2_20221124_RW_BucketSize_30' created successfully.\n",
      "File 'INC11_Ext_20221021_RW_BucketSize_30' created successfully.\n"
     ]
    }
   ],
   "source": [
    "subjectPD = []\n",
    "for eachSubject in subjectFiles:\n",
    "    filesNeeded = [v for v in filenames if eachSubject in v]\n",
    "    \n",
    "    d_list = []\n",
    "    fields = ['Time', 'Movement','Supplemental','Cues']\n",
    "    \n",
    "    for file in filesNeeded:\n",
    "        fileNumber = file.rsplit('_')[-1]\n",
    "        fileNumber = fileNumber.rsplit('.')[0]\n",
    "\n",
    "        #get subject name\n",
    "        subjectFile = file.rsplit('\\\\')[-1]\n",
    "        Subject = subjectFile.rsplit('_')[0]\n",
    "        # if numbered then file is a subsequent sequence else first sequence\n",
    "    \n",
    "        if '-' in fileNumber:\n",
    "            fileNumber = fileNumber.split('-')[1]\n",
    "        else:\n",
    "            fileNumber = 1\n",
    "        #only include data abover first blank row (delete summary data at bottom of set)\n",
    "        d = pd.read_csv(file, header=0, usecols=fields, skip_blank_lines=False)\n",
    "        blank_df = d.loc[d.isnull().all(1)]\n",
    "        if len(blank_df) > 0:\n",
    "            first_blank_index = blank_df.index[0]\n",
    "            d = d[:first_blank_index]\n",
    "        \n",
    "        d['Sequence'] = int(fileNumber)\n",
    "        d['Subject'] = Subject\n",
    "        d['Time'] = d['Time'].astype('float')\n",
    "\n",
    "        d_list.append(d)\n",
    "    d=pd.concat(d_list, axis=0, ignore_index=True)\n",
    "    # Order by Sequence and time and replace time column to start from zero and increase by .2 incr\n",
    "    d=d.sort_values(by=['Sequence', 'Time'], ascending=[True, True]).reset_index(drop=True)\n",
    "\n",
    "    d['CumCount'] = .2\n",
    "    d['Time'] = d['CumCount'].cumsum()-.2\n",
    "\n",
    "    #read csv\n",
    "    \n",
    "    d['BucketOrder'] = d.loc[:,'Time'] / getBucket+1\n",
    "    d['BucketOrder'] = np.floor(d.BucketOrder).astype('int')\n",
    "    #only include timepoints with movement\n",
    "    d = d.dropna(subset=['Movement'])\n",
    "    d[\"TotalTime\"] = d.loc[:,'Time'].diff().round(1)\n",
    "    #remove starting times as not separated by .2\n",
    "    removeGapTime = d.loc[:,\"TotalTime\"] == 0.2\n",
    "    d = d[removeGapTime]\n",
    "\n",
    "    #insert a zero bucket if there is not a bucket\n",
    "    \n",
    "    hasBuckets = d['BucketOrder'].unique()\n",
    "    maxBucket = d['BucketOrder'].max()\n",
    "    missingBucket = np.arange(1, maxBucket+1)\n",
    "    missingBucket = np.setxor1d(hasBuckets, missingBucket)\n",
    "    missingBucket = np.concatenate((hasBuckets, missingBucket))\n",
    "    missingBucket=pd.DataFrame(missingBucket, columns=['Bucket_Updated']).sort_values(['Bucket_Updated'])\n",
    "    #cross join with all movements\n",
    "    missingBucket['key']=1\n",
    "    AllMovements = pd.DataFrame(d['Movement'].unique(), columns = ['AllMovements'])\n",
    "    AllMovements['key']=1\n",
    "    missingBucket=pd.merge(AllMovements, missingBucket, on='key').drop('key', axis=1)\n",
    "    d=d.merge(missingBucket, how='outer', left_on=['BucketOrder', 'Movement'], right_on=['Bucket_Updated', 'AllMovements']).fillna(0).sort_values(['Bucket_Updated', 'Time'])\n",
    "\n",
    "    d['Bucket_Updated'] = d['Bucket_Updated'].astype('int')\n",
    "    d['Bucket'] = ((d.Bucket_Updated-1)*getBucket).astype(str)+\"-\"+(d.Bucket_Updated*getBucket).astype(str)\n",
    "    \n",
    "    # collect only columns of interest and group sums by buckets\n",
    "    d = d.loc[:,['Bucket_Updated', 'Bucket', 'AllMovements', 'TotalTime']]\n",
    "    d = d.groupby(['Bucket_Updated', 'Bucket','AllMovements']).sum()\n",
    "    #drop bucket order\n",
    "    d = d.reset_index()\n",
    "    d = d.drop('Bucket_Updated', axis=1)\n",
    "\n",
    "    #round to nearest tens\n",
    "    d['TotalTime'] = d['TotalTime'].round(2)\n",
    "\n",
    "    #add subject\n",
    "    d['Subject'] = Subject\n",
    "\n",
    "    #make data wide\n",
    "    makeWide = d.pivot(index=['Subject', 'AllMovements'], columns='Bucket', values='TotalTime')\n",
    "    cols=d['Bucket'].unique()\n",
    "    d = makeWide[cols]\n",
    "\n",
    "    #export file\n",
    "    outputName = filesNeeded[-1].split('\\\\')[-1]\n",
    "    outputName = outputName.split('.')[0]\n",
    "    outputName = outputName.split('-')[0]\n",
    "    outputName = outputName +\"_BucketSize_\" +str(getBucket)\n",
    "    d.to_csv(outputName+'.csv', index=True)\n",
    "    print(f\"File '{outputName}' created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FC_Binned_Solomon_Coder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
